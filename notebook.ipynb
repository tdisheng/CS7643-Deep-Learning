{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anthonyivan/anaconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/anthonyivan/anaconda3/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch # for models\n",
    "from torch.nn import Module, Linear, ReLU, LogSoftmax, Conv2d, CrossEntropyLoss\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import pathlib\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, Subset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(Module):\n",
    "    def __init__(self, nClasses):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = Conv2d(in_channels=3, out_channels=16, kernel_size=3)\n",
    "        self.relu1 = ReLU()\n",
    "        self.logSoftmax = LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        output = self.logSoftmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "path = pathlib.Path(\"Rice_Image_Dataset\")\n",
    "dataset = datasets.ImageFolder(path, transform=preprocess)\n",
    "\n",
    "# dataset loader\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Number of classes\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "# train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, (0.7, 0.2, 0.1))\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# Get the target values (labels) from the dataset\n",
    "targets = np.array(dataset.targets)\n",
    "\n",
    "val_prop = 0.2\n",
    "test_prop = 0.1\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_val_indices, test_indices, train_val_targets, test_targets = train_test_split(np.arange(len(dataset)), targets, test_size=test_prop, stratify=targets)\n",
    "\n",
    "# Split the train set into train and validation sets\n",
    "train_indices, val_indices, train_targets, val_targets = train_test_split(train_val_indices, train_val_targets, test_size=val_prop, stratify=train_val_targets)\n",
    "\n",
    "# Create custom PyTorch datasets for the train, validation, and test sets using the original dataset and the indices of the split data\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "# Create custom dataloaders for the train, validation, and test sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, device, lr=1e-3, momentum=0.9, num_classes=5, epochs=1):\n",
    "    # Freeze the weights of the model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model_in_features = model.fc.in_features\n",
    "    model.fc = torch.nn.Linear(model_in_features, num_classes).to(device)\n",
    "    optimizer = torch.optim.SGD([\n",
    "                        {'params': model.fc.parameters()}\n",
    "                    ],\n",
    "                    lr=lr,\n",
    "                    momentum=momentum\n",
    "                )\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        print(f'running epoch {i+1}')\n",
    "        for inputs, targets in train_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model\n",
    "\n",
    "def get_model_specs(model):\n",
    "    total_params = 0 #default value\n",
    "    total_params = sum(\n",
    "        param.numel() for param in model.parameters()\n",
    "    )\n",
    "    return total_params\n",
    "\n",
    "def evaluate_model(model, train_dataloader, val_dataloader, test_dataloader, num_classes=5):\n",
    "    print('collecting param count')\n",
    "    total_params = get_model_specs(model)\n",
    "    print('collecting train accuracy')\n",
    "    train_acc = get_acc(model=model, dataloader=train_dataloader, num_classes=num_classes)\n",
    "    print('collecting validation accuracy')\n",
    "    val_acc = get_acc(model=model, dataloader=val_dataloader, num_classes=num_classes)\n",
    "    print('collecting test accuracy')\n",
    "    test_acc = get_acc(model=model, dataloader=test_dataloader, num_classes=num_classes)\n",
    "    metrics_dict = {\n",
    "        'total_params': total_params,\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'test_acc': test_acc\n",
    "    }\n",
    "    metrics_idx = list(metrics_dict.keys())\n",
    "    metrics = pd.Series(data=metrics_dict, index=metrics_idx)\n",
    "    return metrics\n",
    "\n",
    "def get_acc(model, dataloader, num_classes):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            # Move the inputs and labels to the device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            preds = F.softmax(outputs, dim=1)\n",
    "\n",
    "            # Store the predictions and targets\n",
    "            predictions.extend(preds.cpu().detach().numpy())\n",
    "            targets.extend(labels.cpu().detach().numpy())\n",
    "    accuracy = torchmetrics.functional.accuracy(torch.tensor(predictions), torch.tensor(targets), num_classes=num_classes, task='multiclass')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anthonyivan/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/anthonyivan/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running epoch 1\n",
      "running epoch 2\n",
      "running epoch 3\n",
      "running epoch 4\n",
      "running epoch 5\n",
      "running epoch 6\n",
      "running epoch 7\n",
      "running epoch 8\n",
      "collecting param count\n",
      "collecting train accuracy\n"
     ]
    }
   ],
   "source": [
    "# model loading\n",
    "resnet_model = torchvision.models.resnet50(pretrained=True).to(device)\n",
    "# alexnet_model = torchvision.models.alexnet(pretrained=True).to(device)\n",
    "# vgg_model = torchvision.models.vgg16(pretrained=True).to(device)\n",
    "resnet_model_trained = train_model(resnet_model, train_dataloader=train_dataloader, device=device, num_classes=NUM_CLASSES, epochs=8)\n",
    "metrics = evaluate_model(resnet_model_trained, train_dataloader, val_dataloader, test_dataloader, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metrics\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

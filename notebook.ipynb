{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2953e932b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch # for models\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import pathlib\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, Subset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,number_of_classes):\n",
    "        super().__init__() #Inheritance\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=16,padding=1,kernel_size=3)\n",
    "        self.bn1=nn.BatchNorm2d(num_features=16)\n",
    "        self.relu1=nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3)\n",
    "        self.fc=nn.Linear(in_features=87616, out_features=number_of_classes)\n",
    "    \n",
    "    def forward(self, input):            \n",
    "        output=self.conv1(input)\n",
    "        output=self.bn1(output)\n",
    "        output=self.relu1(output)\n",
    "        output=self.pool1(output)\n",
    "        output = torch.flatten(output, 1)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "path = pathlib.Path(\"data/Rice_Image_Dataset\")\n",
    "dataset = datasets.ImageFolder(path, transform=preprocess)\n",
    "\n",
    "# dataset loader\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Number of classes\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "# train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, (0.7, 0.2, 0.1))\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# Get the target values (labels) from the dataset\n",
    "targets = np.array(dataset.targets)\n",
    "\n",
    "val_prop = 0.2\n",
    "test_prop = 0.1\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_val_indices, test_indices, train_val_targets, test_targets = train_test_split(np.arange(len(dataset)), targets, test_size=test_prop, stratify=targets)\n",
    "\n",
    "# Split the train set into train and validation sets\n",
    "train_indices, val_indices, train_targets, val_targets = train_test_split(train_val_indices, train_val_targets, test_size=val_prop, stratify=train_val_targets)\n",
    "\n",
    "# Create custom PyTorch datasets for the train, validation, and test sets using the original dataset and the indices of the split data\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "# Create custom dataloaders for the train, validation, and test sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Create custom dataloaders for one/few shot learning\n",
    "class OneShotDataset(Dataset):\n",
    "    def __init__(self, path, num_classes=5, transform=preprocess):\n",
    "        self.path = path\n",
    "        self.num_classes = num_classes\n",
    "        self.transform = transform\n",
    "        self.dataset = datasets.ImageFolder(root=path, transform=transform)\n",
    "        self.indices = self._get_one_shot_indices()\n",
    "        \n",
    "    def _get_one_shot_indices(self):\n",
    "        indices = [[] for _ in range(self.num_classes)]\n",
    "        for idx, (image, label) in enumerate(self.dataset):\n",
    "            indices[label].append(idx)\n",
    "        return indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_classes\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch = []\n",
    "        labels = []\n",
    "        for class_idx in range(self.num_classes):\n",
    "            sample_idx = random.choice(self.indices[class_idx])\n",
    "            sample, label = self.dataset[sample_idx]\n",
    "            batch.append(sample)\n",
    "            labels.append(label)\n",
    "        batch = torch.stack(batch, dim=0)\n",
    "        labels = torch.LongTensor(labels)\n",
    "        return batch, labels\n",
    "\n",
    "one_shot_dataset = OneShotDataset(\"data/Rice_Image_Dataset/\")\n",
    "\n",
    "train_dataloader_1 = DataLoader(one_shot_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(batch, batch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 has unique labels: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "batch, labels = next(iter(train_dataloader_1))\n",
    "print(f\"Batch 1 has unique labels: {labels.unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_shots(model, shots=1, train_dataloader=train_dataloader_1, val_dataloader=val_dataloader, device=device, lr=1e-3, momentum=0.9, num_classes=5, epochs=1):\n",
    "    # one/few shot learning\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    if hasattr(model, 'fc'):\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes).to(device)\n",
    "        optimizer = torch.optim.SGD([\n",
    "            {'params': model.fc.parameters()}\n",
    "        ], lr=lr, momentum=momentum)\n",
    "    elif hasattr(model, 'classifier'):\n",
    "        in_features = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(in_features, num_classes).to(device)\n",
    "        optimizer = torch.optim.SGD([\n",
    "            {'params': model.classifier[-1].parameters()}\n",
    "        ], lr=lr, momentum=momentum)\n",
    "    else:\n",
    "        raise ValueError('Model has no fc or classifier attribute')\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    counter = 0\n",
    "    for inputs, targets in train_dataloader:\n",
    "        targets = targets#.squeeze(0)\n",
    "        inputs = inputs#.squeeze(0)\n",
    "        counter += 1\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_loss = 0.0\n",
    "            train_acc = 0.0\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            train_acc += torch.sum(predictions == targets.data)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss /= 5\n",
    "            train_acc /= 5\n",
    "            val_acc, val_loss = get_acc(model=model, dataloader=val_dataloader, num_classes=num_classes)\n",
    "            print('Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.4f}, Val Loss: {:.4f}, Val Acc: {:.4f}'\n",
    "                    .format(epoch+1, epochs, train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "        if counter == shots:\n",
    "            break\n",
    "\n",
    "        \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, device=device, lr=1e-3, momentum=0.9, num_classes=5, epochs=1):\n",
    "    train_acc_lst = []\n",
    "    val_acc_lst = []\n",
    "    train_loss_lst = []\n",
    "    val_loss_lst = []\n",
    "    # Freeze the weights of the model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    try:\n",
    "        model_in_features = model.fc.in_features\n",
    "        model.fc = torch.nn.Linear(model_in_features, num_classes).to(device)\n",
    "        optimizer = torch.optim.SGD([\n",
    "                        {'params': model.fc.parameters()}\n",
    "                    ],\n",
    "                    lr=lr,\n",
    "                    momentum=momentum\n",
    "                )\n",
    "    except:\n",
    "        model_in_features = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = torch.nn.Linear(model_in_features, num_classes).to(device)\n",
    "        optimizer = torch.optim.SGD([\n",
    "                            {'params': model.classifier[-1].parameters()}\n",
    "                        ],\n",
    "                        lr=lr,\n",
    "                        momentum=momentum\n",
    "                    )\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(f'running epoch {epoch+1}')\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        for inputs, targets in train_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            train_acc += torch.sum(predictions == targets.data)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /= len(train_dataloader.dataset)\n",
    "        train_acc /= len(train_dataloader.dataset)\n",
    "        val_acc, val_loss = get_acc(model=model, dataloader=val_dataloader, num_classes=num_classes)\n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.4f}, Val Loss: {:.4f}, Val Acc: {:.4f}'\n",
    "              .format(epoch+1, epochs, train_loss, train_acc, val_loss, val_acc))\n",
    "        train_acc_lst.append(train_acc)\n",
    "        val_acc_lst.append(val_acc)\n",
    "        train_loss_lst.append(train_loss)\n",
    "        val_loss_lst.append(val_loss)\n",
    "    training_metrics_result = {\n",
    "        'train_acc': train_acc_lst,\n",
    "        'val_acc': val_acc_lst,\n",
    "        'train_loss': train_loss_lst,\n",
    "        'val_loss': val_loss_lst\n",
    "    }\n",
    "    return model\n",
    "\n",
    "def get_model_specs(model):\n",
    "    total_params = 0 #default value\n",
    "    total_params = sum(\n",
    "        param.numel() for param in model.parameters()\n",
    "    )\n",
    "    return total_params\n",
    "\n",
    "def evaluate_model(model, train_dataloader, val_dataloader, test_dataloader, num_classes=5):\n",
    "    print('collecting param count')\n",
    "    total_params = get_model_specs(model)\n",
    "    print('collecting train accuracy')\n",
    "    train_acc, train_loss = get_acc(model=model, dataloader=train_dataloader, num_classes=num_classes)\n",
    "    print('collecting validation accuracy')\n",
    "    val_acc, val_loss = get_acc(model=model, dataloader=val_dataloader, num_classes=num_classes)\n",
    "    print('collecting test accuracy')\n",
    "    test_acc, test_loss = get_acc(model=model, dataloader=test_dataloader, num_classes=num_classes)\n",
    "    metrics_dict = {\n",
    "        'total_params': total_params,\n",
    "        'train_acc': train_acc,\n",
    "        'train_loss': train_loss,\n",
    "        'val_acc': val_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'test_acc': test_acc,\n",
    "        'test_loss': test_loss\n",
    "    }\n",
    "    metrics_idx = list(metrics_dict.keys())\n",
    "    metrics = pd.Series(data=metrics_dict, index=metrics_idx)\n",
    "    return metrics\n",
    "\n",
    "def get_acc(model, dataloader, num_classes):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            # Move the inputs and labels to the device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            preds = F.softmax(outputs, dim=1)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            # Store the predictions and targets\n",
    "            predictions.extend(preds.cpu().detach().numpy())\n",
    "            targets.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "    # Calculate the accuracy and average loss\n",
    "    accuracy = torchmetrics.functional.accuracy(torch.tensor(predictions), torch.tensor(targets), num_classes=num_classes, task='multiclass')\n",
    "    avg_loss = total_loss / total_samples\n",
    "    \n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disheng/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/disheng/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/home/disheng/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/disheng/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/disheng/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training not pretrained resnet model\n",
      "tensor([0, 1, 2, 3, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_504/724179154.py:160: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  accuracy = torchmetrics.functional.accuracy(torch.tensor(predictions), torch.tensor(targets), num_classes=num_classes, task='multiclass')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 1.6168, Train Acc: 0.0000, Val Loss: 1.6092, Val Acc: 0.2674\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [2/10], Train Loss: 1.6094, Train Acc: 0.2000, Val Loss: 1.6092, Val Acc: 0.2714\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [3/10], Train Loss: 1.6060, Train Acc: 0.4000, Val Loss: 1.6091, Val Acc: 0.2750\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [4/10], Train Loss: 1.6078, Train Acc: 0.2000, Val Loss: 1.6091, Val Acc: 0.2764\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [5/10], Train Loss: 1.6089, Train Acc: 0.2000, Val Loss: 1.6090, Val Acc: 0.2736\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [6/10], Train Loss: 1.6088, Train Acc: 0.2000, Val Loss: 1.6090, Val Acc: 0.2725\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [7/10], Train Loss: 1.6104, Train Acc: 0.2000, Val Loss: 1.6089, Val Acc: 0.2651\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [8/10], Train Loss: 1.6106, Train Acc: 0.2000, Val Loss: 1.6088, Val Acc: 0.2541\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [9/10], Train Loss: 1.6125, Train Acc: 0.2000, Val Loss: 1.6088, Val Acc: 0.2337\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [10/10], Train Loss: 1.6100, Train Acc: 0.2000, Val Loss: 1.6087, Val Acc: 0.2133\n",
      "training not pretrained alexnet model\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [1/10], Train Loss: 1.6101, Train Acc: 0.2000, Val Loss: 1.6095, Val Acc: 0.1939\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [2/10], Train Loss: 1.6096, Train Acc: 0.2000, Val Loss: 1.6095, Val Acc: 0.1939\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [3/10], Train Loss: 1.6095, Train Acc: 0.2000, Val Loss: 1.6095, Val Acc: 0.1939\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [4/10], Train Loss: 1.6096, Train Acc: 0.2000, Val Loss: 1.6095, Val Acc: 0.1940\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [5/10], Train Loss: 1.6094, Train Acc: 0.2000, Val Loss: 1.6095, Val Acc: 0.1940\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [6/10], Train Loss: 1.6097, Train Acc: 0.2000, Val Loss: 1.6095, Val Acc: 0.1940\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [7/10], Train Loss: 1.6095, Train Acc: 0.2000, Val Loss: 1.6095, Val Acc: 0.1941\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [8/10], Train Loss: 1.6096, Train Acc: 0.2000, Val Loss: 1.6095, Val Acc: 0.1941\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [9/10], Train Loss: 1.6094, Train Acc: 0.2000, Val Loss: 1.6095, Val Acc: 0.1941\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [10/10], Train Loss: 1.6093, Train Acc: 0.2000, Val Loss: 1.6095, Val Acc: 0.1941\n",
      "training not pretrained resnet model\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [1/10], Train Loss: 1.6656, Train Acc: 0.2000, Val Loss: 4.3792, Val Acc: 0.2000\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [2/10], Train Loss: 4.3408, Train Acc: 0.2000, Val Loss: 3.0747, Val Acc: 0.2028\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [3/10], Train Loss: 3.2871, Train Acc: 0.2000, Val Loss: 4.5770, Val Acc: 0.1996\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [4/10], Train Loss: 4.7889, Train Acc: 0.2000, Val Loss: 5.4776, Val Acc: 0.2122\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [5/10], Train Loss: 5.0163, Train Acc: 0.0000, Val Loss: 5.4422, Val Acc: 0.2000\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [6/10], Train Loss: 5.2922, Train Acc: 0.2000, Val Loss: 4.5654, Val Acc: 0.1902\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [7/10], Train Loss: 4.2769, Train Acc: 0.0000, Val Loss: 5.0621, Val Acc: 0.2000\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [8/10], Train Loss: 5.0777, Train Acc: 0.2000, Val Loss: 2.8073, Val Acc: 0.2148\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [9/10], Train Loss: 2.6891, Train Acc: 0.4000, Val Loss: 5.8711, Val Acc: 0.2000\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [10/10], Train Loss: 6.0245, Train Acc: 0.2000, Val Loss: 7.7115, Val Acc: 0.2000\n",
      "training pretrained resnet model\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [1/10], Train Loss: 1.8916, Train Acc: 0.2000, Val Loss: 1.6640, Val Acc: 0.2585\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [2/10], Train Loss: 1.5948, Train Acc: 0.4000, Val Loss: 1.6187, Val Acc: 0.2726\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [3/10], Train Loss: 1.7886, Train Acc: 0.0000, Val Loss: 1.5547, Val Acc: 0.3075\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [4/10], Train Loss: 1.5905, Train Acc: 0.2000, Val Loss: 1.4941, Val Acc: 0.3822\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [5/10], Train Loss: 1.6053, Train Acc: 0.0000, Val Loss: 1.4519, Val Acc: 0.4185\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [6/10], Train Loss: 1.4847, Train Acc: 0.4000, Val Loss: 1.4241, Val Acc: 0.4353\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [7/10], Train Loss: 1.5200, Train Acc: 0.4000, Val Loss: 1.4034, Val Acc: 0.4392\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [8/10], Train Loss: 1.4488, Train Acc: 0.2000, Val Loss: 1.3824, Val Acc: 0.4354\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [9/10], Train Loss: 1.4554, Train Acc: 0.4000, Val Loss: 1.3577, Val Acc: 0.4226\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [10/10], Train Loss: 1.3737, Train Acc: 0.2000, Val Loss: 1.3216, Val Acc: 0.4449\n",
      "training pretrained alexnet model\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [1/10], Train Loss: 1.8354, Train Acc: 0.0000, Val Loss: 1.5899, Val Acc: 0.2073\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [2/10], Train Loss: 1.7286, Train Acc: 0.0000, Val Loss: 1.4676, Val Acc: 0.3387\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [3/10], Train Loss: 1.3994, Train Acc: 0.4000, Val Loss: 1.3624, Val Acc: 0.3975\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [4/10], Train Loss: 1.3976, Train Acc: 0.2000, Val Loss: 1.1802, Val Acc: 0.6422\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [5/10], Train Loss: 1.0759, Train Acc: 0.8000, Val Loss: 1.0183, Val Acc: 0.7401\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [6/10], Train Loss: 0.9497, Train Acc: 0.6000, Val Loss: 0.9049, Val Acc: 0.7801\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [7/10], Train Loss: 0.8883, Train Acc: 0.8000, Val Loss: 0.8003, Val Acc: 0.8083\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [8/10], Train Loss: 0.7441, Train Acc: 0.6000, Val Loss: 0.7000, Val Acc: 0.8287\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [9/10], Train Loss: 0.7632, Train Acc: 0.8000, Val Loss: 0.6401, Val Acc: 0.7973\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [10/10], Train Loss: 0.4944, Train Acc: 1.0000, Val Loss: 0.5865, Val Acc: 0.8087\n",
      "training pretrained vgg model\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [1/10], Train Loss: 1.6706, Train Acc: 0.6000, Val Loss: 1.5439, Val Acc: 0.3530\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [2/10], Train Loss: 1.4960, Train Acc: 0.4000, Val Loss: 1.5092, Val Acc: 0.4037\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [3/10], Train Loss: 1.5208, Train Acc: 0.2000, Val Loss: 1.4713, Val Acc: 0.3809\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [4/10], Train Loss: 1.4178, Train Acc: 0.6000, Val Loss: 1.4256, Val Acc: 0.4086\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [5/10], Train Loss: 1.4916, Train Acc: 0.4000, Val Loss: 1.3673, Val Acc: 0.5193\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [6/10], Train Loss: 1.4922, Train Acc: 0.4000, Val Loss: 1.3001, Val Acc: 0.6497\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [7/10], Train Loss: 1.2161, Train Acc: 0.8000, Val Loss: 1.2347, Val Acc: 0.7145\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [8/10], Train Loss: 1.1564, Train Acc: 0.8000, Val Loss: 1.1751, Val Acc: 0.7144\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [9/10], Train Loss: 1.1294, Train Acc: 0.8000, Val Loss: 1.1214, Val Acc: 0.7016\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [10/10], Train Loss: 1.1334, Train Acc: 0.6000, Val Loss: 1.0707, Val Acc: 0.7045\n",
      "training not pretrained cnn model\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [1/10], Train Loss: 1.6167, Train Acc: 0.2000, Val Loss: 1.6787, Val Acc: 0.2012\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [2/10], Train Loss: 1.8385, Train Acc: 0.2000, Val Loss: 1.4675, Val Acc: 0.4221\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [3/10], Train Loss: 1.4454, Train Acc: 0.4000, Val Loss: 1.3964, Val Acc: 0.3804\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [4/10], Train Loss: 1.4834, Train Acc: 0.6000, Val Loss: 1.3206, Val Acc: 0.4340\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [5/10], Train Loss: 1.3500, Train Acc: 0.6000, Val Loss: 1.1841, Val Acc: 0.5136\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [6/10], Train Loss: 1.2750, Train Acc: 0.4000, Val Loss: 1.2219, Val Acc: 0.4640\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [7/10], Train Loss: 1.3544, Train Acc: 0.6000, Val Loss: 1.1210, Val Acc: 0.5205\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [8/10], Train Loss: 1.0469, Train Acc: 0.4000, Val Loss: 1.0077, Val Acc: 0.6293\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [9/10], Train Loss: 1.2906, Train Acc: 0.4000, Val Loss: 0.9705, Val Acc: 0.6079\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "Epoch [10/10], Train Loss: 0.8811, Train Acc: 0.4000, Val Loss: 1.0136, Val Acc: 0.5480\n"
     ]
    }
   ],
   "source": [
    "# model loading\n",
    "not_pretrained_resnet_model = torchvision.models.resnet34(pretrained=False).to(device)\n",
    "not_pretrained_alexnet_model = torchvision.models.alexnet(pretrained=False).to(device)\n",
    "not_pretrained_vgg_model = torchvision.models.vgg16(pretrained=False).to(device)\n",
    "pretrained_resnet_model = torchvision.models.resnet34(pretrained=True).to(device)\n",
    "pretrained_alexnet_model = torchvision.models.alexnet(pretrained=True).to(device)\n",
    "pretrained_vgg_model = torchvision.models.vgg16(pretrained=True).to(device)\n",
    "not_pretrained_cnn_model = CNN(5).to(device)\n",
    "\n",
    "# model training\n",
    "print('training not pretrained resnet model')\n",
    "not_pretrained_vgg_model_trained_one_shot = train_model_shots(not_pretrained_vgg_model, shots=1, train_dataloader=train_dataloader_1, val_dataloader=val_dataloader, device=device, num_classes=NUM_CLASSES, epochs=10)\n",
    "print('training not pretrained alexnet model')\n",
    "not_pretrained_alexnet_model_trained_one_shot = train_model_shots(not_pretrained_alexnet_model, shots=1, train_dataloader=train_dataloader_1, val_dataloader=val_dataloader, device=device, num_classes=NUM_CLASSES, epochs=10)\n",
    "print('training not pretrained resnet model')\n",
    "not_pretrained_resnet_model_trained_one_shot = train_model_shots(not_pretrained_resnet_model, shots=1, train_dataloader=train_dataloader_1, val_dataloader=val_dataloader, device=device, num_classes=NUM_CLASSES, epochs=10)\n",
    "print('training pretrained resnet model')\n",
    "pretrained_resnet_model_trained_one_shot = train_model_shots(pretrained_resnet_model, shots=1, train_dataloader=train_dataloader_1, val_dataloader=val_dataloader, device=device, num_classes=NUM_CLASSES, epochs=10)\n",
    "print('training pretrained alexnet model')\n",
    "pretrained_alexnet_model_trained_one_shot = train_model_shots(pretrained_alexnet_model, shots=1, train_dataloader=train_dataloader_1, val_dataloader=val_dataloader, device=device, num_classes=NUM_CLASSES, epochs=10)\n",
    "print('training pretrained vgg model')\n",
    "pretrained_vgg_model_trained_one_shot = train_model_shots(pretrained_vgg_model, shots=1, train_dataloader=train_dataloader_1, val_dataloader=val_dataloader, device=device, num_classes=NUM_CLASSES, epochs=10)\n",
    "print('training not pretrained cnn model')\n",
    "not_pretrained_cnn_model_trained_one_shot = train_model_shots(not_pretrained_cnn_model, shots=1, train_dataloader=train_dataloader_1, val_dataloader=val_dataloader, device=device, num_classes=NUM_CLASSES, epochs=10)\n",
    "\n",
    "# metrics_not_pretrained_vgg_model_one_shot = evaluate_model(not_pretrained_vgg_model_trained_one_shot, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "# metrics_not_pretrained_alexnet_model_one_shot = evaluate_model(not_pretrained_alexnet_model_trained_one_shot, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "# metrics_not_pretrained_resnet_model_one_shot = evaluate_model(not_pretrained_resnet_model_trained_one_shot, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "# metrics_pretrained_resnet_model_one_shot = evaluate_model(pretrained_resnet_model_trained_one_shot, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "# metrics_pretrained_alexnet_model_one_shot = evaluate_model(pretrained_alexnet_model_trained_one_shot, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "# metrics_pretrained_vgg_model_one_shot = evaluate_model(pretrained_vgg_model_trained_one_shot, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "# metrics_not_pretrained_cnn_model_one_shot = evaluate_model(not_pretrained_cnn_model_trained_one_shot, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "\n",
    "# # print metrics\n",
    "# print(\"Not pretrained VGG model trained one shot\")\n",
    "# print(metrics_not_pretrained_vgg_model_one_shot)\n",
    "# print(\"Not pretrained Alexnet model trained one shot\")\n",
    "# print(metrics_not_pretrained_alexnet_model_one_shot)\n",
    "# print(\"Not pretrained Resnet model trained one shot\")\n",
    "# print(metrics_not_pretrained_resnet_model_one_shot)\n",
    "# print(\"Pretrained Resnet model trained one shot\")\n",
    "# print(metrics_pretrained_resnet_model_one_shot)\n",
    "# print(\"Pretrained Alexnet model trained one shot\")\n",
    "# print(metrics_pretrained_alexnet_model_one_shot)\n",
    "# print(\"Pretrained VGG model trained one shot\")\n",
    "# print(metrics_pretrained_vgg_model_one_shot)\n",
    "# print(\"Not pretrained CNN model trained one shot\")\n",
    "# print(metrics_not_pretrained_cnn_model_one_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loading\n",
    "not_pretrained_resnet_model = torchvision.models.resnet34(pretrained=False).to(device)\n",
    "not_pretrained_alexnet_model = torchvision.models.alexnet(pretrained=False).to(device)\n",
    "not_pretrained_vgg_model = torchvision.models.vgg16(pretrained=False).to(device)\n",
    "pretrained_resnet_model = torchvision.models.resnet34(pretrained=True).to(device)\n",
    "pretrained_alexnet_model = torchvision.models.alexnet(pretrained=True).to(device)\n",
    "pretrained_vgg_model = torchvision.models.vgg16(pretrained=True).to(device)\n",
    "\n",
    "# model training\n",
    "not_pretrained_vgg_model_trained_five_shots = train_model_shots(not_pretrained_vgg_model, shots=5, train_dataloader=train_dataloader_1, val_dataloader=val_dataloader, device=device, num_classes=NUM_CLASSES)\n",
    "not_pretrained_alexnet_model_trained_five_shots = train_model_shots(not_pretrained_alexnet_model, shots=5, train_dataloader=train_dataloader_1, val_dataloader=val_dataloader, device=device, num_classes=NUM_CLASSES)\n",
    "not_pretrained_resnet_model_trained_five_shots = train_model_shots(not_pretrained_resnet_model, shots=5, train_dataloader=train_dataloader_1, val_dataloader=val_dataloader, device=device, num_classes=NUM_CLASSES)\n",
    "pretrained_resnet_model_trained_five_shots = train_model_shots(pretrained_resnet_model, shots=5, train_dataloader=train_dataloader_1, val_dataloader=val_dataloader, device=device, num_classes=NUM_CLASSES)\n",
    "pretrained_alexnet_model_trained_five_shots = train_model_shots(pretrained_alexnet_model, shots=5, train_dataloader=train_dataloader_1, val_dataloader=val_dataloader, device=device, num_classes=NUM_CLASSES)\n",
    "pretrained_vgg_model_trained_five_shots = train_model_shots(pretrained_vgg_model, shots=5, train_dataloader=train_dataloader_1, val_dataloader=val_dataloader, device=device, num_classes=NUM_CLASSES)\n",
    "\n",
    "metrics_not_pretrained_vgg_model_five_shots = evaluate_model(not_pretrained_vgg_model_trained_five_shots, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "metrics_not_pretrained_alexnet_model_five_shots = evaluate_model(not_pretrained_alexnet_model_trained_five_shots, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "metrics_not_pretrained_resnet_model_five_shots = evaluate_model(not_pretrained_resnet_model_trained_five_shots, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "metrics_pretrained_resnet_model_five_shots = evaluate_model(pretrained_resnet_model_trained_five_shots, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "metrics_pretrained_alexnet_model_five_shots = evaluate_model(pretrained_alexnet_model_trained_five_shots, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "metrics_pretrained_vgg_model_five_shots = evaluate_model(pretrained_vgg_model_trained_five_shots, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "\n",
    "# print metrics\n",
    "print(\"Not pretrained VGG model trained one shot\")\n",
    "print(metrics_not_pretrained_vgg_model_five_shots)\n",
    "print(\"Not pretrained Alexnet model trained one shot\")\n",
    "print(metrics_not_pretrained_alexnet_model_five_shots)\n",
    "print(\"Not pretrained Resnet model trained one shot\")\n",
    "print(metrics_not_pretrained_resnet_model_five_shots)\n",
    "print(\"Pretrained Resnet model trained one shot\")\n",
    "print(metrics_pretrained_resnet_model_five_shots)\n",
    "print(\"Pretrained Alexnet model trained one shot\")\n",
    "print(metrics_pretrained_alexnet_model_five_shots)\n",
    "print(\"Pretrained VGG model trained one shot\")\n",
    "print(metrics_pretrained_vgg_model_five_shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loading\n",
    "not_pretrained_resnet_model = torchvision.models.resnet34(pretrained=False).to(device)\n",
    "not_pretrained_alexnet_model = torchvision.models.alexnet(pretrained=False).to(device)\n",
    "not_pretrained_vgg_model = torchvision.models.vgg16(pretrained=False).to(device)\n",
    "pretrained_resnet_model = torchvision.models.resnet34(pretrained=True).to(device)\n",
    "pretrained_alexnet_model = torchvision.models.alexnet(pretrained=True).to(device)\n",
    "pretrained_vgg_model = torchvision.models.vgg16(pretrained=True).to(device)\n",
    "not_pretrained_cnn_model = CNN(5).to(device)\n",
    "\n",
    "# model training\n",
    "not_pretrained_vgg_model_trained = train_model(not_pretrained_vgg_model, train_dataloader=train_dataloader, device=device, num_classes=NUM_CLASSES, epochs=10)\n",
    "not_pretrained_alexnet_model_trained = train_model(not_pretrained_alexnet_model, train_dataloader=train_dataloader, device=device, num_classes=NUM_CLASSES, epochs=10)\n",
    "not_pretrained_resnet_model_trained = train_model(not_pretrained_resnet_model, train_dataloader=train_dataloader, device=device, num_classes=NUM_CLASSES, epochs=10)\n",
    "pretrained_resnet_model_trained = train_model(pretrained_resnet_model, train_dataloader=train_dataloader, device=device, num_classes=NUM_CLASSES, epochs=10)\n",
    "pretrained_alexnet_model_trained = train_model(pretrained_alexnet_model, train_dataloader=train_dataloader, device=device, num_classes=NUM_CLASSES, epochs=10)\n",
    "pretrained_vgg_model_trained = train_model(pretrained_vgg_model, train_dataloader=train_dataloader, device=device, num_classes=NUM_CLASSES, epochs=10)\n",
    "not_pretrained_cnn_model_trained = train_model(not_pretrained_cnn_model, train_dataloader=train_dataloader, device=device, num_classes=NUM_CLASSES, epochs=10)\n",
    "\n",
    "metrics_not_pretrained_vgg_model = evaluate_model(not_pretrained_vgg_model_trained, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "metrics_not_pretrained_alexnet_model = evaluate_model(not_pretrained_alexnet_model_trained, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "metrics_not_pretrained_resnet_model = evaluate_model(not_pretrained_resnet_model_trained, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "metrics_pretrained_resnet_model = evaluate_model(pretrained_resnet_model_trained, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "metrics_pretrained_alexnet_model = evaluate_model(pretrained_alexnet_model_trained, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "metrics_pretrained_vgg_model = evaluate_model(pretrained_vgg_model_trained, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "metrics_not_pretrained_cnn_model = evaluate_model(not_pretrained_cnn_model_trained, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "\n",
    "# print metrics\n",
    "print(\"Not pretrained VGG model trained one shot\")\n",
    "print(metrics_not_pretrained_vgg_model)\n",
    "print(\"Not pretrained Alexnet model trained one shot\")\n",
    "print(metrics_not_pretrained_alexnet_model)\n",
    "print(\"Not pretrained Resnet model trained one shot\")\n",
    "print(metrics_not_pretrained_resnet_model)\n",
    "print(\"Pretrained Resnet model trained one shot\")\n",
    "print(metrics_pretrained_resnet_model)\n",
    "print(\"Pretrained Alexnet model trained one shot\")\n",
    "print(metrics_pretrained_alexnet_model)\n",
    "print(\"Pretrained VGG model trained one shot\")\n",
    "print(metrics_pretrained_vgg_model)\n",
    "print(\"Not pretrained CNN model trained one shot\")\n",
    "print(metrics_not_pretrained_cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5819/3538882782.py:177: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  accuracy = torchmetrics.functional.accuracy(torch.tensor(predictions), torch.tensor(targets), num_classes=num_classes, task='multiclass')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Train Loss: 0.3851, Train Acc: 0.9294, Val Loss: 0.0600, Val Acc: 0.9812\n",
      "running epoch 2\n",
      "Epoch [2/8], Train Loss: 0.0423, Train Acc: 0.9864, Val Loss: 0.0503, Val Acc: 0.9857\n",
      "running epoch 3\n",
      "Epoch [3/8], Train Loss: 0.0343, Train Acc: 0.9892, Val Loss: 0.0420, Val Acc: 0.9878\n",
      "running epoch 4\n",
      "Epoch [4/8], Train Loss: 0.0300, Train Acc: 0.9905, Val Loss: 0.0394, Val Acc: 0.9879\n",
      "running epoch 5\n",
      "Epoch [5/8], Train Loss: 0.0241, Train Acc: 0.9927, Val Loss: 0.0418, Val Acc: 0.9875\n",
      "running epoch 6\n",
      "Epoch [6/8], Train Loss: 0.0215, Train Acc: 0.9937, Val Loss: 0.0377, Val Acc: 0.9892\n",
      "running epoch 7\n",
      "Epoch [7/8], Train Loss: 0.0190, Train Acc: 0.9945, Val Loss: 0.0363, Val Acc: 0.9892\n",
      "running epoch 8\n",
      "Epoch [8/8], Train Loss: 0.0178, Train Acc: 0.9951, Val Loss: 0.0371, Val Acc: 0.9896\n",
      "collecting param count\n",
      "collecting train accuracy\n",
      "collecting validation accuracy\n",
      "collecting test accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "total_params            438565\n",
       "train_acc       tensor(0.9963)\n",
       "train_loss            0.015114\n",
       "val_acc         tensor(0.9896)\n",
       "val_loss              0.037128\n",
       "test_acc        tensor(0.9892)\n",
       "test_loss              0.03267\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train CNN\n",
    "cnn_model = CNN(5).to(device)\n",
    "cnn_model_trained = train_model(cnn_model, train_dataloader=train_dataloader, device=device, num_classes=NUM_CLASSES, epochs=8)\n",
    "metrics_cnn = evaluate_model(cnn_model_trained, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "metrics_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code copied from assignment 3, with modifications to how it is being used\n",
    "class SaliencyMap:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "    def compute_saliency_maps(self, X, y, model):\n",
    "        \"\"\"\n",
    "        Compute a class saliency map using the model for images X and labels y.\n",
    "\n",
    "        Input:\n",
    "        - X: Input images; Tensor of shape (N, 3, H, W)\n",
    "        - y: Labels for X; LongTensor of shape (N,)\n",
    "        - model: A pretrained CNN that will be used to compute the saliency map.\n",
    "\n",
    "        Returns:\n",
    "        - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input\n",
    "        images.\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # Wrap the input tensors in Variables\n",
    "        X_var = Variable(X, requires_grad=True).to(self.device)\n",
    "        y_var = Variable(y, requires_grad=False).to(self.device)\n",
    "        saliency = None\n",
    "\n",
    "        output = model(X_var)\n",
    "        grad_outputs = torch.FloatTensor(output.size(0), output.size()[-1]).zero_().to(self.device)\n",
    "        for i in range(grad_outputs.size()[0]):\n",
    "            grad_outputs[i][int(y_var[i].item())] = 1\n",
    "        output = output * grad_outputs\n",
    "        grad_vals = torch.abs(torch.tensor(torch.autograd.grad(outputs=output.sum(), inputs=X_var)[0]))\n",
    "        saliency_vals = []\n",
    "        for grad_val in grad_vals:\n",
    "            curr_grad_val = torch.max(grad_val, dim=0)[0]\n",
    "            saliency_vals.append(curr_grad_val)\n",
    "        saliency = torch.stack(saliency_vals)\n",
    "        return saliency\n",
    "\n",
    "    def show_saliency_maps(self, X, y, labels, model, folder=\"visualization\", filename='saliency_map_visualization.png'):\n",
    "        # Convert X and y from numpy arrays to Torch Tensors\n",
    "        X_tensor = torch.Tensor(X).to(self.device)\n",
    "        y_tensor = torch.Tensor(y).to(self.device)\n",
    "\n",
    "        # Compute saliency maps for images in X\n",
    "        saliency = self.compute_saliency_maps(X_tensor, y_tensor, model)\n",
    "        # Convert the saliency map from Torch Tensor to numpy array and show images\n",
    "        # and saliency maps together.\n",
    "        saliency = saliency.cpu().numpy()\n",
    "\n",
    "        # Create a figure and a subplot with 2 rows and 4 columns\n",
    "        fig, ax = plt.subplots(2, 5, figsize=(12, 6))\n",
    "        fig.subplots_adjust(left=0.03, right=0.97, bottom=0.03, top=0.92, wspace=0.2, hspace=0.2)\n",
    "\n",
    "        # Loop over the subplots and plot an image in each one\n",
    "        for i in tqdm(range(2), desc=\"Creating plots\", leave=True):\n",
    "            for j in tqdm(range(5), desc=\"Processing image\", leave=True):\n",
    "                # Load image\n",
    "                if i == 0:\n",
    "                    image = X[j]\n",
    "                elif i == 1:\n",
    "                    image = saliency[j]\n",
    "                image_data = np.transpose(image_data, (1, 2, 0))\n",
    "                # Plot the image in the current subplot\n",
    "                ax[i, j].imshow(image)\n",
    "                ax[i, j].axis('off')\n",
    "\n",
    "                # Add a label above each image in the bottom row\n",
    "                if i == 1:\n",
    "                    ax[i, j].set_title(labels[j].title(), fontsize=12, y=1.2)\n",
    "\n",
    "        # Save and display the subplots\n",
    "        plt.savefig(f\"./{folder}/{filename}\")\n",
    "        plt.show()\n",
    "\n",
    "def generate_samples(dataset, images_per_class=1):\n",
    "    dataset_class_labels = np.array(dataset.classes)\n",
    "    dataset_classes_vals = np.array(dataset.targets)\n",
    "    dataset_classes_unique = list(set(dataset.targets))\n",
    "    input_data_sample = []\n",
    "    target_sample = []\n",
    "    class_label_sample = []\n",
    "    for class_val, class_label in zip(dataset_classes_unique, dataset_class_labels):\n",
    "        class_val_idxs = [i for i, x in enumerate(dataset_classes_vals == class_val) if x]\n",
    "        curr_class_idx_samples = np.random.choice(class_val_idxs, size=images_per_class)\n",
    "        for idx in curr_class_idx_samples:\n",
    "            input_data, target = dataset[idx]\n",
    "            input_data_sample.append(input_data.numpy())\n",
    "            target_sample.append(target)\n",
    "            class_label_sample.append(class_label)\n",
    "    return input_data_sample, target_sample, class_label_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_sample, target_sample, class_label_sample = generate_samples(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5819/2050307238.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  grad_vals = torch.abs(torch.tensor(torch.autograd.grad(outputs=output.sum(), inputs=X_var)[0]))\n",
      "Processing image:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Creating plots:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (3, 224, 224) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m salience_map \u001b[39m=\u001b[39m SaliencyMap(device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m----> 2\u001b[0m salience_map\u001b[39m.\u001b[39;49mshow_saliency_maps(X\u001b[39m=\u001b[39;49minput_data_sample, y\u001b[39m=\u001b[39;49mtarget_sample, labels\u001b[39m=\u001b[39;49mclass_label_sample, model\u001b[39m=\u001b[39;49mcnn_model_trained)\n",
      "Cell \u001b[0;32mIn[25], line 63\u001b[0m, in \u001b[0;36mSaliencyMap.show_saliency_maps\u001b[0;34m(self, X, y, labels, model, folder, filename)\u001b[0m\n\u001b[1;32m     60\u001b[0m     image \u001b[39m=\u001b[39m saliency[j]\n\u001b[1;32m     62\u001b[0m \u001b[39m# Plot the image in the current subplot\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m ax[i, j]\u001b[39m.\u001b[39;49mimshow(image)\n\u001b[1;32m     64\u001b[0m ax[i, j]\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[39m# Add a label above each image in the bottom row\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/__init__.py:1442\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m   1440\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1441\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1442\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1444\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1445\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[1;32m   1446\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py:5665\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5657\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m   5658\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm,\n\u001b[1;32m   5659\u001b[0m                       interpolation\u001b[39m=\u001b[39minterpolation, origin\u001b[39m=\u001b[39morigin,\n\u001b[1;32m   5660\u001b[0m                       extent\u001b[39m=\u001b[39mextent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[1;32m   5661\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[1;32m   5662\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[1;32m   5663\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 5665\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[1;32m   5666\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5667\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5668\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/image.py:710\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A[:, :, \u001b[39m0\u001b[39m]\n\u001b[1;32m    708\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    709\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m]):\n\u001b[0;32m--> 710\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m for image data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    711\u001b[0m                     \u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape))\n\u001b[1;32m    713\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m    714\u001b[0m     \u001b[39m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    715\u001b[0m     \u001b[39m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    716\u001b[0m     \u001b[39m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    717\u001b[0m     \u001b[39m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    718\u001b[0m     high \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger) \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (3, 224, 224) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "salience_map = SaliencyMap(device=device)\n",
    "salience_map.show_saliency_maps(X=input_data_sample, y=target_sample, labels=class_label_sample, model=cnn_model_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

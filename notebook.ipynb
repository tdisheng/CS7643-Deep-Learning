{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch # for models\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import pathlib\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, Subset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,number_of_classes):\n",
    "        super().__init__() #Inheritance\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=16,padding=1,kernel_size=3)\n",
    "        self.bn1=nn.BatchNorm2d(num_features=16)\n",
    "        self.relu1=nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=16,out_channels=32,padding=1,kernel_size=3)\n",
    "        self.bn2=nn.BatchNorm2d(num_features=32)\n",
    "        self.relu2=nn.ReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=32,out_channels=64,padding=1,kernel_size=3)\n",
    "        self.bn3=nn.BatchNorm2d(num_features=64)\n",
    "        self.relu3=nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.fc=nn.Linear(64*56*56, number_of_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, Input):\n",
    "            \n",
    "        output=self.conv1(Input)\n",
    "        output=self.bn1(output)\n",
    "        output=self.relu1(output)\n",
    "        output=self.pool1(output)\n",
    "        \n",
    "        output=self.conv2(output)\n",
    "        output=self.bn2(output)\n",
    "        output=self.relu2(output)\n",
    "        \n",
    "        output=self.conv3(output)\n",
    "        output=self.bn3(output)\n",
    "        output=self.relu3(output)\n",
    "        output=self.pool3(output)\n",
    "        \n",
    "        output = torch.flatten(output, 1)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "path = pathlib.Path(\"data/Rice_Image_Dataset\")\n",
    "dataset = datasets.ImageFolder(path, transform=preprocess)\n",
    "\n",
    "# dataset loader\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Number of classes\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "# train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, (0.7, 0.2, 0.1))\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# Get the target values (labels) from the dataset\n",
    "targets = np.array(dataset.targets)\n",
    "\n",
    "val_prop = 0.2\n",
    "test_prop = 0.1\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_val_indices, test_indices, train_val_targets, test_targets = train_test_split(np.arange(len(dataset)), targets, test_size=test_prop, stratify=targets)\n",
    "\n",
    "# Split the train set into train and validation sets\n",
    "train_indices, val_indices, train_targets, val_targets = train_test_split(train_val_indices, train_val_targets, test_size=val_prop, stratify=train_val_targets)\n",
    "\n",
    "# Create custom PyTorch datasets for the train, validation, and test sets using the original dataset and the indices of the split data\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "# Create custom dataloaders for the train, validation, and test sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, device=device, lr=1e-3, momentum=0.9, num_classes=5, epochs=1):\n",
    "    # Freeze the weights of the model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model_in_features = model.fc.in_features\n",
    "    model.fc = torch.nn.Linear(model_in_features, num_classes).to(device)\n",
    "    optimizer = torch.optim.SGD([\n",
    "                        {'params': model.fc.parameters()}\n",
    "                    ],\n",
    "                    lr=lr,\n",
    "                    momentum=momentum\n",
    "                )\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(f'running epoch {epoch+1}')\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        for inputs, targets in train_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            train_acc += torch.sum(predictions == targets.data)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /= len(train_dataloader.dataset)\n",
    "        train_acc /= len(train_dataloader.dataset)\n",
    "        val_loss, val_acc = get_acc(model=model, dataloader=val_dataloader, num_classes=num_classes)\n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.4f}, Val Loss: {:.4f}, Val Acc: {:.4f}'\n",
    "              .format(epoch+1, epochs, train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_model_specs(model):\n",
    "    total_params = 0 #default value\n",
    "    total_params = sum(\n",
    "        param.numel() for param in model.parameters()\n",
    "    )\n",
    "    return total_params\n",
    "\n",
    "def evaluate_model(model, train_dataloader, val_dataloader, test_dataloader, num_classes=5):\n",
    "    print('collecting param count')\n",
    "    total_params = get_model_specs(model)\n",
    "    print('collecting train accuracy')\n",
    "    train_acc, train_loss = get_acc(model=model, dataloader=train_dataloader, num_classes=num_classes)\n",
    "    print('collecting validation accuracy')\n",
    "    val_acc, val_loss = get_acc(model=model, dataloader=val_dataloader, num_classes=num_classes)\n",
    "    print('collecting test accuracy')\n",
    "    test_acc, test_loss = get_acc(model=model, dataloader=test_dataloader, num_classes=num_classes)\n",
    "    metrics_dict = {\n",
    "        'total_params': total_params,\n",
    "        'train_acc': train_acc,\n",
    "        'train_loss': train_loss,\n",
    "        'val_acc': val_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'test_acc': test_acc,\n",
    "        'test_loss': test_loss\n",
    "    }\n",
    "    metrics_idx = list(metrics_dict.keys())\n",
    "    metrics = pd.Series(data=metrics_dict, index=metrics_idx)\n",
    "    return metrics\n",
    "\n",
    "def get_acc(model, dataloader, num_classes):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            # Move the inputs and labels to the device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            preds = F.softmax(outputs, dim=1)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            # Store the predictions and targets\n",
    "            predictions.extend(preds.cpu().detach().numpy())\n",
    "            targets.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "    # Calculate the accuracy and average loss\n",
    "    accuracy = torchmetrics.functional.accuracy(torch.tensor(predictions), torch.tensor(targets), num_classes=num_classes, task='multiclass')\n",
    "    avg_loss = total_loss / total_samples\n",
    "    \n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loading\n",
    "# resnet_model = torchvision.models.resnet34(pretrained=True).to(device)\n",
    "# alexnet_model = torchvision.models.alexnet(pretrained=True).to(device)\n",
    "# vgg_model = torchvision.models.vgg16(pretrained=True).to(device)\n",
    "# resnet_model_trained = train_model(resnet_model, train_dataloader=train_dataloader, device=device, num_classes=NUM_CLASSES, epochs=8)\n",
    "# metrics = evaluate_model(resnet_model_trained, train_dataloader, val_dataloader, test_dataloader, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running epoch 1\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'total_loss' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# train CNN\u001b[39;00m\n\u001b[1;32m      2\u001b[0m cnn_model \u001b[39m=\u001b[39m CNN(\u001b[39m5\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m cnn_model_trained \u001b[39m=\u001b[39m train_model(cnn_model, train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader, device\u001b[39m=\u001b[39;49mdevice, num_classes\u001b[39m=\u001b[39;49mNUM_CLASSES, epochs\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m metrics_cnn \u001b[39m=\u001b[39m evaluate_model(cnn_model_trained, train_dataloader, val_dataloader, test_dataloader, \u001b[39m5\u001b[39m)\n\u001b[1;32m      5\u001b[0m metrics_cnn\n",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, device, lr, momentum, num_classes, epochs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     train_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_dataloader\u001b[39m.\u001b[39mdataset)\n\u001b[1;32m     34\u001b[0m     train_acc \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_dataloader\u001b[39m.\u001b[39mdataset)\n\u001b[0;32m---> 35\u001b[0m     val_loss, val_acc \u001b[39m=\u001b[39m get_acc(model\u001b[39m=\u001b[39;49mmodel, dataloader\u001b[39m=\u001b[39;49mval_dataloader, num_classes\u001b[39m=\u001b[39;49mnum_classes)\n\u001b[1;32m     36\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m], Train Loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m, Train Acc: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m, Val Loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m, Val Acc: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     37\u001b[0m           \u001b[39m.\u001b[39mformat(epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, epochs, train_loss, train_acc, val_loss, val_acc))\n\u001b[1;32m     39\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[6], line 86\u001b[0m, in \u001b[0;36mget_acc\u001b[0;34m(model, dataloader, num_classes)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39m# Calculate the loss\u001b[39;00m\n\u001b[1;32m     85\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mcross_entropy(outputs, labels)\n\u001b[0;32m---> 86\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     87\u001b[0m total_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[39m# Store the predictions and targets\u001b[39;00m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'total_loss' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# train CNN\n",
    "cnn_model = CNN(5).to(device)\n",
    "cnn_model_trained = train_model(cnn_model, train_dataloader=train_dataloader, device=device, num_classes=NUM_CLASSES, epochs=8)\n",
    "metrics_cnn = evaluate_model(cnn_model_trained, train_dataloader, val_dataloader, test_dataloader, 5)\n",
    "metrics_cnn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
